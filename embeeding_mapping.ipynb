{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad24bbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\anaconda3\\envs\\onionAI\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-23 12:11:32,712 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
      "2025-04-23 12:11:32,712 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2025-04-23 12:11:37,085 - SentenceMapper - INFO - 다국어 임베딩 모델 'paraphrase-multilingual-MiniLM-L12-v2' 로드 완료\n",
      "2025-04-23 12:11:37,085 - SentenceMapper - INFO - 문장 매핑 시스템 초기화 완료\n",
      "2025-04-23 12:11:37,095 - SentenceMapper - INFO - TextGrid에서 5개 문장 추출 완료\n",
      "2025-04-23 12:11:37,096 - SentenceMapper - INFO - TextGrid에서 6개 문장 추출 완료\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.89it/s]\n",
      "2025-04-23 12:11:37,196 - SentenceMapper - INFO - 5개 문장의 임베딩 계산 완료\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.97it/s]\n",
      "2025-04-23 12:11:37,231 - SentenceMapper - INFO - 6개 문장의 임베딩 계산 완료\n",
      "2025-04-23 12:11:37,257 - SentenceMapper - INFO - 임계값 0.4 이상의 매핑 5개 찾음\n",
      "2025-04-23 12:11:37,257 - SentenceMapper - INFO - 매핑: (유사도 0.8957)\n",
      "2025-04-23 12:11:37,257 - SentenceMapper - INFO -   소스: Sentence(0): 안녕 (1.03s ~ 1.28s)\n",
      "2025-04-23 12:11:37,262 - SentenceMapper - INFO -   타겟: Sentence(0): hello (0.00s ~ 0.78s)\n",
      "2025-04-23 12:11:37,262 - SentenceMapper - INFO - 매핑: (유사도 0.5999)\n",
      "2025-04-23 12:11:37,263 - SentenceMapper - INFO -   소스: Sentence(2): 이름 은 조윤장 입니다 (2.65s ~ 4.50s)\n",
      "2025-04-23 12:11:37,264 - SentenceMapper - INFO -   타겟: Sentence(1): my name is jo yoon jang (1.41s ~ 3.05s)\n",
      "2025-04-23 12:11:37,264 - SentenceMapper - INFO - 매핑: (유사도 0.6107)\n",
      "2025-04-23 12:11:37,264 - SentenceMapper - INFO -   소스: Sentence(3): 만나 서 반갑 습니다 (5.44s ~ 6.99s)\n",
      "2025-04-23 12:11:37,264 - SentenceMapper - INFO -   타겟: Sentence(3): to meet you (6.10s ~ 7.07s)\n",
      "2025-04-23 12:11:37,264 - SentenceMapper - INFO - 매핑: (유사도 0.7548)\n",
      "2025-04-23 12:11:37,264 - SentenceMapper - INFO -   소스: Sentence(4): 잘 부탁 드립니다 (7.87s ~ 9.11s)\n",
      "2025-04-23 12:11:37,264 - SentenceMapper - INFO -   타겟: Sentence(4): please (7.71s ~ 8.40s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Isochrony Score: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 12:11:37,921 - SentenceMapper - INFO - 시각화 결과를 textgrid_mappings.png에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "다국어 임베딩 기반 문장 매핑 시스템\n",
    "\n",
    "이 모듈은 한국어와 영어 스크립트를 의미적 유사성을 기반으로 매핑하는 기능을 제공합니다.\n",
    "문장 임베딩 벡터를 사용해 언어 간 경계를 넘어 의미적으로 유사한 문장을 찾습니다.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Optional, Set, Any\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import logging\n",
    "from textgrid import TextGrid, IntervalTier\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"SentenceMapper\")\n",
    "\n",
    "#####################################################\n",
    "# 1. 문장 관리 및 전처리 모듈\n",
    "#####################################################\n",
    "\n",
    "@dataclass\n",
    "class Sentence:\n",
    "    \"\"\"문장 정보를 저장하는 데이터 클래스\"\"\"\n",
    "    text: str  # 문장 텍스트\n",
    "    words: List[str]  # 단어 리스트\n",
    "    start_time: float = -1  # 시작 시간 (초)\n",
    "    end_time: float = -1  # 종료 시간 (초)\n",
    "    index: int = -1  # 원본 스크립트에서의 인덱스\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not self.words and self.text:\n",
    "            # 텍스트만 제공된 경우 단어 리스트 생성\n",
    "            self.words = self.text.split()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Sentence({self.index}): {self.text} ({self.start_time:.2f}s ~ {self.end_time:.2f}s)\"\n",
    "\n",
    "\n",
    "class SentencePreprocessor:\n",
    "    \"\"\"문장 전처리 및 정규화를 담당하는 클래스\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"텍스트 정규화: 공백 정리, 특수문자 처리 등\"\"\"\n",
    "        # 여러 공백을 하나로 줄이기\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # 양쪽 공백 제거\n",
    "        text = text.strip()\n",
    "        # 소문자로 변환 (선택사항)\n",
    "        # text = text.lower()\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def split_into_sentences(text: str, lang: str = 'ko') -> List[str]:\n",
    "        \"\"\"\n",
    "        텍스트를 문장 단위로 분리\n",
    "        \n",
    "        Args:\n",
    "            text: 분리할 텍스트\n",
    "            lang: 언어 코드 ('ko': 한국어, 'en': 영어)\n",
    "            \n",
    "        Returns:\n",
    "            문장 리스트\n",
    "        \"\"\"\n",
    "        # 간단한 문장 분리 규칙 (언어별로 다른 규칙 적용 가능)\n",
    "        if lang == 'ko':\n",
    "            # 한국어 문장 분리 규칙\n",
    "            sentence_pattern = r'([^.!?…]+[.!?…]+)'\n",
    "        else:\n",
    "            # 영어 문장 분리 규칙\n",
    "            sentence_pattern = r'([^.!?]+[.!?]+)'\n",
    "        \n",
    "        sentences = re.findall(sentence_pattern, text)\n",
    "        \n",
    "        # 정규식으로 찾지 못한 마지막 부분이 있다면 추가\n",
    "        leftover = re.sub(sentence_pattern, '', text).strip()\n",
    "        if leftover:\n",
    "            sentences.append(leftover)\n",
    "        \n",
    "        # 정규화 적용\n",
    "        return [SentencePreprocessor.normalize_text(s) for s in sentences if s.strip()]\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_sentences_from_textgrid(\n",
    "        textgrid_path: str, \n",
    "        tier_name: str = \"words\", \n",
    "        min_pause: float = 0.5\n",
    "    ) -> List[Sentence]:\n",
    "        \"\"\"\n",
    "        TextGrid 파일에서 문장 단위로 단어들을 추출\n",
    "        \n",
    "        Args:\n",
    "            textgrid_path: TextGrid 파일 경로\n",
    "            tier_name: 추출할 계층 이름\n",
    "            min_pause: 문장 구분을 위한 최소 휴지 시간(초)\n",
    "            \n",
    "        Returns:\n",
    "            Sentence 객체 리스트\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tg = TextGrid.fromFile(textgrid_path)\n",
    "            tier = tg.getFirst(tier_name)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"TextGrid 파일 로드 실패: {e}\")\n",
    "            return []\n",
    "            \n",
    "        sentences = []\n",
    "        current_words = []\n",
    "        word_times = []\n",
    "        last_end_time = 0\n",
    "        sentence_idx = 0\n",
    "        \n",
    "        for interval in tier:\n",
    "            # 빈 mark는 건너뜀\n",
    "            mark = interval.mark.strip()\n",
    "            if not mark:\n",
    "                continue\n",
    "                \n",
    "            # 긴 휴지(pause) 감지\n",
    "            if interval.minTime - last_end_time > min_pause and current_words:\n",
    "                # 현재까지의 단어로 문장 생성\n",
    "                sentence_text = ' '.join(current_words)\n",
    "                start_time = word_times[0][0] if word_times else 0\n",
    "                end_time = word_times[-1][1] if word_times else 0\n",
    "                \n",
    "                sentences.append(Sentence(\n",
    "                    text=sentence_text,\n",
    "                    words=current_words.copy(),\n",
    "                    start_time=start_time,\n",
    "                    end_time=end_time,\n",
    "                    index=sentence_idx\n",
    "                ))\n",
    "                sentence_idx += 1\n",
    "                \n",
    "                # 초기화\n",
    "                current_words = []\n",
    "                word_times = []\n",
    "            \n",
    "            # 현재 단어 추가\n",
    "            current_words.append(mark)\n",
    "            word_times.append((interval.minTime, interval.maxTime))\n",
    "            last_end_time = interval.maxTime\n",
    "        \n",
    "        # 마지막 문장 처리\n",
    "        if current_words:\n",
    "            sentence_text = ' '.join(current_words)\n",
    "            start_time = word_times[0][0] if word_times else 0\n",
    "            end_time = word_times[-1][1] if word_times else 0\n",
    "            \n",
    "            sentences.append(Sentence(\n",
    "                text=sentence_text,\n",
    "                words=current_words,\n",
    "                start_time=start_time,\n",
    "                end_time=end_time,\n",
    "                index=sentence_idx\n",
    "            ))\n",
    "        \n",
    "        logger.info(f\"TextGrid에서 {len(sentences)}개 문장 추출 완료\")\n",
    "        return sentences\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 2. 다국어 임베딩 모듈\n",
    "#####################################################\n",
    "\n",
    "class MultilingualEmbedder:\n",
    "    \"\"\"다국어 임베딩 모델을 관리하는 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "        \"\"\"\n",
    "        다국어 임베딩 모델 초기화\n",
    "        \n",
    "        Args:\n",
    "            model_name: 사용할 Sentence-Transformers 모델 이름\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            logger.info(f\"다국어 임베딩 모델 '{model_name}' 로드 완료\")\n",
    "        except ImportError:\n",
    "            logger.error(\"sentence-transformers 라이브러리가 설치되지 않았습니다.\")\n",
    "            logger.error(\"pip install sentence-transformers 명령어로 설치하세요.\")\n",
    "            raise\n",
    "\n",
    "    def encode_sentences(self, sentences: List[Sentence]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        문장 리스트를 임베딩 벡터로 변환\n",
    "        \n",
    "        Args:\n",
    "            sentences: Sentence 객체 리스트\n",
    "            \n",
    "        Returns:\n",
    "            문장 임베딩 벡터 배열 (shape: n_sentences x embedding_dim)\n",
    "        \"\"\"\n",
    "        texts = [s.text for s in sentences]\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        logger.info(f\"{len(texts)}개 문장의 임베딩 계산 완료\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 3. 유사도 계산 및 매핑 모듈\n",
    "#####################################################\n",
    "\n",
    "class SentenceMapper:\n",
    "    \"\"\"문장 간 유사도 계산 및 최적 매핑을 찾는 클래스\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_similarity_matrix(\n",
    "        source_embeddings: np.ndarray, \n",
    "        target_embeddings: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        두 임베딩 집합 간의 코사인 유사도 행렬 계산\n",
    "        \n",
    "        Args:\n",
    "            source_embeddings: 소스 문장 임베딩 (shape: n_source x embedding_dim)\n",
    "            target_embeddings: 타겟 문장 임베딩 (shape: n_target x embedding_dim)\n",
    "            \n",
    "        Returns:\n",
    "            유사도 행렬 (shape: n_source x n_target)\n",
    "        \"\"\"\n",
    "        # 임베딩 정규화\n",
    "        source_norm = np.linalg.norm(source_embeddings, axis=1, keepdims=True)\n",
    "        target_norm = np.linalg.norm(target_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # 0으로 나누기 방지\n",
    "        source_norm = np.where(source_norm == 0, 1e-10, source_norm)\n",
    "        target_norm = np.where(target_norm == 0, 1e-10, target_norm)\n",
    "        \n",
    "        # 정규화된 임베딩\n",
    "        source_normalized = source_embeddings / source_norm\n",
    "        target_normalized = target_embeddings / target_norm\n",
    "        \n",
    "        # 코사인 유사도 행렬 계산 (내적)\n",
    "        similarity_matrix = np.dot(source_normalized, target_normalized.T)\n",
    "        \n",
    "        return similarity_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def find_optimal_mapping(\n",
    "        similarity_matrix: np.ndarray, \n",
    "        threshold: float = 0.5\n",
    "    ) -> List[Tuple[int, int, float]]:\n",
    "        \"\"\"\n",
    "        헝가리안 알고리즘을 사용해 최적의 매핑 찾기\n",
    "        \n",
    "        Args:\n",
    "            similarity_matrix: 유사도 행렬\n",
    "            threshold: 최소 유사도 임계값\n",
    "            \n",
    "        Returns:\n",
    "            매핑 리스트 [(source_idx, target_idx, similarity), ...]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from scipy.optimize import linear_sum_assignment\n",
    "        except ImportError:\n",
    "            logger.error(\"scipy 라이브러리가 설치되지 않았습니다.\")\n",
    "            logger.error(\"pip install scipy 명령어로 설치하세요.\")\n",
    "            raise\n",
    "            \n",
    "        # 유사도가 높을수록 좋으므로 음수로 변환하여 비용 행렬로 만듦\n",
    "        cost_matrix = -similarity_matrix\n",
    "        \n",
    "        # 헝가리안 알고리즘으로 최적 매핑 찾기\n",
    "        source_indices, target_indices = linear_sum_assignment(cost_matrix)\n",
    "        \n",
    "        # 결과 형식화 및 임계값 필터링\n",
    "        mappings = []\n",
    "        for source_idx, target_idx in zip(source_indices, target_indices):\n",
    "            similarity = similarity_matrix[source_idx, target_idx]\n",
    "            if similarity >= threshold:\n",
    "                mappings.append((source_idx, target_idx, similarity))\n",
    "        \n",
    "        # 유사도 기준 내림차순 정렬\n",
    "        mappings.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        logger.info(f\"임계값 {threshold} 이상의 매핑 {len(mappings)}개 찾음\")\n",
    "        return mappings\n",
    "\n",
    "    @staticmethod\n",
    "    def refine_mappings(\n",
    "        mappings: List[Tuple[int, int, float]], \n",
    "        source_sentences: List[Sentence], \n",
    "        target_sentences: List[Sentence],\n",
    "        sequential: bool = True\n",
    "    ) -> List[Tuple[int, int, float]]:\n",
    "        \"\"\"\n",
    "        매핑 결과 정제\n",
    "        \n",
    "        Args:\n",
    "            mappings: 원본 매핑 리스트\n",
    "            source_sentences: 소스 문장 리스트\n",
    "            target_sentences: 타겟 문장 리스트\n",
    "            sequential: 순차적 매핑 강제 여부\n",
    "            \n",
    "        Returns:\n",
    "            정제된 매핑 리스트\n",
    "        \"\"\"\n",
    "        if not mappings:\n",
    "            return []\n",
    "            \n",
    "        if sequential:\n",
    "            # 원본 순서를 유지하는 방향으로 정제\n",
    "            refined = []\n",
    "            used_source = set()\n",
    "            used_target = set()\n",
    "            \n",
    "            # 유사도 순으로 정렬된 매핑에서 시작\n",
    "            for source_idx, target_idx, similarity in mappings:\n",
    "                # 이미 사용된 인덱스는 건너뜀\n",
    "                if source_idx in used_source or target_idx in used_target:\n",
    "                    continue\n",
    "                    \n",
    "                # 이전에 추가된 매핑이 있는지 확인\n",
    "                if refined:\n",
    "                    last_source, last_target, _ = refined[-1]\n",
    "                    \n",
    "                    # 순서가 뒤집히는 경우 건너뜀\n",
    "                    if (source_idx < last_source and target_idx > last_target) or \\\n",
    "                       (source_idx > last_source and target_idx < last_target):\n",
    "                        continue\n",
    "                \n",
    "                refined.append((source_idx, target_idx, similarity))\n",
    "                used_source.add(source_idx)\n",
    "                used_target.add(target_idx)\n",
    "            \n",
    "            # 원본 문장 순서대로 정렬\n",
    "            refined.sort(key=lambda x: x[0])\n",
    "            return refined\n",
    "        \n",
    "        return mappings\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 4. 통합 문장 매핑 시스템\n",
    "#####################################################\n",
    "\n",
    "class SentenceMappingSystem:\n",
    "    \"\"\"문장 매핑 시스템 통합 클래스\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str = 'paraphrase-multilingual-MiniLM-L12-v2', \n",
    "        similarity_threshold: float = 0.5,\n",
    "        enforce_sequential: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        문장 매핑 시스템 초기화\n",
    "        \n",
    "        Args:\n",
    "            model_name: 사용할 임베딩 모델 이름\n",
    "            similarity_threshold: 매핑 임계값\n",
    "            enforce_sequential: 순차적 매핑 강제 여부\n",
    "        \"\"\"\n",
    "        self.preprocessor = SentencePreprocessor()\n",
    "        self.embedder = MultilingualEmbedder(model_name)\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.enforce_sequential = enforce_sequential\n",
    "        logger.info(\"문장 매핑 시스템 초기화 완료\")\n",
    "        \n",
    "    def map_scripts(\n",
    "        self, \n",
    "        source_text: str, \n",
    "        target_text: str, \n",
    "        source_lang: str = 'ko',\n",
    "        target_lang: str = 'en'\n",
    "    ) -> List[Tuple[Sentence, Sentence, float]]:\n",
    "        \"\"\"\n",
    "        텍스트 스크립트를 문장 단위로 매핑\n",
    "        \n",
    "        Args:\n",
    "            source_text: 소스 텍스트\n",
    "            target_text: 타겟 텍스트\n",
    "            source_lang: 소스 언어 코드\n",
    "            target_lang: 타겟 언어 코드\n",
    "            \n",
    "        Returns:\n",
    "            매핑된 문장 쌍 리스트 [(source_sentence, target_sentence, similarity), ...]\n",
    "        \"\"\"\n",
    "        # 1. 문장 분리\n",
    "        source_sentences_text = self.preprocessor.split_into_sentences(source_text, source_lang)\n",
    "        target_sentences_text = self.preprocessor.split_into_sentences(target_text, target_lang)\n",
    "        \n",
    "        # 2. Sentence 객체 생성\n",
    "        source_sentences = [Sentence(text=s, words=s.split(), index=i) \n",
    "                           for i, s in enumerate(source_sentences_text)]\n",
    "        target_sentences = [Sentence(text=s, words=s.split(), index=i) \n",
    "                           for i, s in enumerate(target_sentences_text)]\n",
    "        \n",
    "        return self._map_sentence_objects(source_sentences, target_sentences)\n",
    "        \n",
    "    def map_textgrids(\n",
    "        self, \n",
    "        source_textgrid_path: str, \n",
    "        target_textgrid_path: str,\n",
    "        source_tier: str = \"words\",\n",
    "        target_tier: str = \"words\",\n",
    "        min_pause: float = 0.5\n",
    "    ) -> List[Tuple[Sentence, Sentence, float]]:\n",
    "        \"\"\"\n",
    "        TextGrid 파일에서 문장을 추출하고 매핑\n",
    "        \n",
    "        Args:\n",
    "            source_textgrid_path: 소스 TextGrid 파일 경로\n",
    "            target_textgrid_path: 타겟 TextGrid 파일 경로\n",
    "            source_tier: 소스 계층 이름\n",
    "            target_tier: 타겟 계층 이름\n",
    "            min_pause: 문장 구분을 위한 최소 휴지 시간(초)\n",
    "            \n",
    "        Returns:\n",
    "            매핑된 문장 쌍 리스트 [(source_sentence, target_sentence, similarity), ...]\n",
    "        \"\"\"\n",
    "        # TextGrid에서 문장 추출\n",
    "        source_sentences = self.preprocessor.extract_sentences_from_textgrid(\n",
    "            source_textgrid_path, source_tier, min_pause)\n",
    "        target_sentences = self.preprocessor.extract_sentences_from_textgrid(\n",
    "            target_textgrid_path, target_tier, min_pause)\n",
    "        \n",
    "        return self._map_sentence_objects(source_sentences, target_sentences)\n",
    "        \n",
    "    def _map_sentence_objects(\n",
    "        self, \n",
    "        source_sentences: List[Sentence], \n",
    "        target_sentences: List[Sentence]\n",
    "    ) -> List[Tuple[Sentence, Sentence, float]]:\n",
    "        \"\"\"\n",
    "        Sentence 객체 리스트 간 매핑 수행\n",
    "        \n",
    "        Args:\n",
    "            source_sentences: 소스 Sentence 객체 리스트\n",
    "            target_sentences: 타겟 Sentence 객체 리스트\n",
    "            \n",
    "        Returns:\n",
    "            매핑된 문장 쌍 리스트 [(source_sentence, target_sentence, similarity), ...]\n",
    "        \"\"\"\n",
    "        if not source_sentences or not target_sentences:\n",
    "            logger.warning(\"빈 문장 리스트가 입력되었습니다.\")\n",
    "            return []\n",
    "            \n",
    "        # 1. 임베딩 계산\n",
    "        source_embeddings = self.embedder.encode_sentences(source_sentences)\n",
    "        target_embeddings = self.embedder.encode_sentences(target_sentences)\n",
    "        \n",
    "        # 2. 유사도 행렬 계산\n",
    "        similarity_matrix = SentenceMapper.compute_similarity_matrix(\n",
    "            source_embeddings, target_embeddings)\n",
    "        \n",
    "        # 3. 최적 매핑 찾기\n",
    "        raw_mappings = SentenceMapper.find_optimal_mapping(\n",
    "            similarity_matrix, self.similarity_threshold)\n",
    "        \n",
    "        # 4. 매핑 정제\n",
    "        refined_mappings = SentenceMapper.refine_mappings(\n",
    "            raw_mappings, source_sentences, target_sentences, self.enforce_sequential)\n",
    "        \n",
    "        # 5. 결과 형식화\n",
    "        results = []\n",
    "        for source_idx, target_idx, similarity in refined_mappings:\n",
    "            source_sentence = source_sentences[source_idx]\n",
    "            target_sentence = target_sentences[target_idx]\n",
    "            results.append((source_sentence, target_sentence, similarity))\n",
    "            \n",
    "            # 로그 출력\n",
    "            logger.info(f\"매핑: (유사도 {similarity:.4f})\")\n",
    "            logger.info(f\"  소스: {source_sentence}\")\n",
    "            logger.info(f\"  타겟: {target_sentence}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def export_mapping_to_csv(\n",
    "        self, \n",
    "        mappings: List[Tuple[Sentence, Sentence, float]], \n",
    "        output_path: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        매핑 결과를 CSV 파일로 내보내기\n",
    "        \n",
    "        Args:\n",
    "            mappings: 매핑 결과 리스트\n",
    "            output_path: 저장할 CSV 파일 경로\n",
    "        \"\"\"\n",
    "        import csv\n",
    "        \n",
    "        with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['Source Index', 'Source Text', 'Source Start', 'Source End', \n",
    "                           'Target Index', 'Target Text', 'Target Start', 'Target End', \n",
    "                           'Similarity'])\n",
    "            \n",
    "            for source, target, similarity in mappings:\n",
    "                writer.writerow([\n",
    "                    source.index, source.text, source.start_time, source.end_time,\n",
    "                    target.index, target.text, target.start_time, target.end_time,\n",
    "                    similarity\n",
    "                ])\n",
    "                \n",
    "        logger.info(f\"매핑 결과를 {output_path}에 저장했습니다.\")\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 5. 응용 예시\n",
    "#####################################################\n",
    "\n",
    "def calculate_isochrony_score(\n",
    "    mappings: List[Tuple[Sentence, Sentence, float]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    매핑된 문장 쌍 리스트에서 isochrony 점수 계산\n",
    "    \n",
    "    Args:\n",
    "        mappings: 매핑된 문장 쌍 리스트\n",
    "        \n",
    "    Returns:\n",
    "        isochrony 점수 (0.0 ~ 1.0)\n",
    "    \"\"\"\n",
    "    if not mappings:\n",
    "        return 0.0\n",
    "        \n",
    "    # 각 매핑된 문장 쌍의 시간 차이 계산\n",
    "    duration_diffs = []\n",
    "    for source, target, _ in mappings:\n",
    "        if source.start_time >= 0 and source.end_time >= 0 and \\\n",
    "           target.start_time >= 0 and target.end_time >= 0:\n",
    "            source_dur = source.end_time - source.start_time\n",
    "            target_dur = target.end_time - target.start_time\n",
    "            diff = abs(source_dur - target_dur)\n",
    "            duration_diffs.append(diff)\n",
    "    \n",
    "    if not duration_diffs:\n",
    "        return 0.0\n",
    "        \n",
    "    # 전체 오디오 길이 추정\n",
    "    max_source_end = max(s.end_time for s, _, _ in mappings)\n",
    "    max_target_end = max(t.end_time for _, t, _ in mappings)\n",
    "    total_duration = max(max_source_end, max_target_end)\n",
    "    \n",
    "    # isochrony 점수 계산\n",
    "    ichron_score = 1.0 - (np.mean(duration_diffs) / total_duration)\n",
    "    \n",
    "    return max(0.0, min(1.0, ichron_score))\n",
    "\n",
    "\n",
    "def visualize_mappings(\n",
    "    mappings: List[Tuple[Sentence, Sentence, float]], \n",
    "    output_path: str = \"sentence_mappings.png\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    매핑 결과 시각화\n",
    "    \n",
    "    Args:\n",
    "        mappings: 매핑된 문장 쌍 리스트\n",
    "        output_path: 저장할 이미지 파일 경로\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, len(mappings)*1.5))\n",
    "        \n",
    "        for i, (source, target, similarity) in enumerate(mappings):\n",
    "            if source.start_time >= 0 and source.end_time >= 0 and \\\n",
    "               target.start_time >= 0 and target.end_time >= 0:\n",
    "                source_dur = source.end_time - source.start_time\n",
    "                target_dur = target.end_time - target.start_time\n",
    "                \n",
    "                # 막대 그래프 그리기\n",
    "                ax.broken_barh([(source.start_time, source_dur)], (i*2.0 + 0.2, 0.6), \n",
    "                               facecolors='tab:blue', label='Source' if i == 0 else \"\")\n",
    "                ax.broken_barh([(target.start_time, target_dur)], (i*2.0 + 1.0, 0.6), \n",
    "                               facecolors='tab:orange', label='Target' if i == 0 else \"\")\n",
    "                \n",
    "                # 문장 정보 표시\n",
    "                ax.text(0, i*2.0 + 0.1, \n",
    "                       f\"Pair {i+1} (sim={similarity:.2f})\", \n",
    "                       fontsize=9, verticalalignment='bottom')\n",
    "        \n",
    "        # 스타일\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        ax.set_title(\"Sentence Alignment Visualization\")\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "        \n",
    "        logger.info(f\"시각화 결과를 {output_path}에 저장했습니다.\")\n",
    "    except ImportError:\n",
    "        logger.error(\"matplotlib 라이브러리가 설치되지 않았습니다.\")\n",
    "        logger.error(\"pip install matplotlib 명령어로 설치하세요.\")\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 6. 사용 예시\n",
    "#####################################################\n",
    "\n",
    "def example_usage_scripts():\n",
    "    \"\"\"스크립트 텍스트 매핑 예시\"\"\"\n",
    "    # 샘플 스크립트\n",
    "    ko_script = \"\"\"\n",
    "    안녕하세요. 제 이름은 조윤장입니다. \n",
    "    만나서 반갑습니다. 잘 부탁드립니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    en_script = \"\"\"\n",
    "    Hello. My name is Jo Yoon Jang.\n",
    "    Nice to meet you. Please take care of me.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 시스템 초기화 및 매핑\n",
    "    system = SentenceMappingSystem(similarity_threshold=0.5)\n",
    "    mappings = system.map_scripts(ko_script, en_script, 'ko', 'en')\n",
    "    \n",
    "    # 결과 출력\n",
    "    for source, target, similarity in mappings:\n",
    "        print(f\"유사도: {similarity:.4f}\")\n",
    "        print(f\"  한국어: {source.text}\")\n",
    "        print(f\"  영어: {target.text}\")\n",
    "        print()\n",
    "    \n",
    "    # CSV로 내보내기\n",
    "    system.export_mapping_to_csv(mappings, \"script_mappings.csv\")\n",
    "\n",
    "\n",
    "def example_usage_textgrids():\n",
    "    \"\"\"TextGrid 파일 매핑 예시\"\"\"\n",
    "    # TextGrid 파일 경로\n",
    "    ko_tg_path = \"./sound_sample/korean_yunjang/윤장목소리1.TextGrid\"\n",
    "    en_tg_path = \"./sound_sample/AI_eng_yunjang/mine_en.TextGrid\"\n",
    "    \n",
    "    # 시스템 초기화 및 매핑\n",
    "    system = SentenceMappingSystem(similarity_threshold=0.4, enforce_sequential=True)\n",
    "    mappings = system.map_textgrids(ko_tg_path, en_tg_path, \"words\", \"words\", 0.4)\n",
    "    \n",
    "    # isochrony 점수 계산\n",
    "    score = calculate_isochrony_score(mappings)\n",
    "    print(f\"\\n✅ Final Isochrony Score: {score:.3f}\")\n",
    "    \n",
    "    # 결과 시각화\n",
    "    visualize_mappings(mappings, \"textgrid_mappings.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 스크립트 매핑 예시 실행\n",
    "    # example_usage_scripts()\n",
    "    \n",
    "    # TextGrid 매핑 예시 실행\n",
    "    example_usage_textgrids()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onionAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
